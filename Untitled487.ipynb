{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80458eed-b277-4f60-bbd9-eb49cd6745a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Loaded Successfully\n",
      "  Sl.No        Month    Year  Count\n",
      "0     1   January     2021.0    500\n",
      "1     2   February    2021.0    549\n",
      "2     3   March       2021.0    618\n",
      "3     4   April       2021.0    435\n",
      "4     5   May         2021.0     74\n",
      "WARNING:tensorflow:From C:\\Users\\NXTWAVE\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\NXTWAVE\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Epoch 1/300\n",
      "WARNING:tensorflow:From C:\\Users\\NXTWAVE\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\NXTWAVE\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "1/1 [==============================] - 2s 2s/step - loss: 245695.0000 - mae: 456.9533 - val_loss: 237222.4062 - val_mae: 484.5352\n",
      "Epoch 2/300\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 245679.4062 - mae: 456.9368 - val_loss: 237210.5938 - val_mae: 484.5223\n",
      "Epoch 3/300\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 245664.3438 - mae: 456.9208 - val_loss: 237198.8750 - val_mae: 484.5096\n",
      "Epoch 4/300\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 245649.3906 - mae: 456.9050 - val_loss: 237187.8906 - val_mae: 484.4975\n",
      "Epoch 5/300\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 245634.4531 - mae: 456.8891 - val_loss: 237177.0000 - val_mae: 484.4855\n",
      "Epoch 6/300\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 245619.2656 - mae: 456.8731 - val_loss: 237166.0625 - val_mae: 484.4734\n",
      "Epoch 7/300\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 245604.0156 - mae: 456.8570 - val_loss: 237155.6094 - val_mae: 484.4618\n",
      "Epoch 8/300\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 245588.7188 - mae: 456.8408 - val_loss: 237145.3750 - val_mae: 484.4503\n",
      "Epoch 9/300\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 245573.4219 - mae: 456.8244 - val_loss: 237135.0000 - val_mae: 484.4388\n",
      "Epoch 10/300\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 245558.3906 - mae: 456.8083 - val_loss: 237124.5312 - val_mae: 484.4271\n",
      "Epoch 11/300\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 245543.6094 - mae: 456.7925 - val_loss: 237114.0625 - val_mae: 484.4154\n",
      "Epoch 12/300\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 245529.0312 - mae: 456.7769 - val_loss: 237103.5469 - val_mae: 484.4037\n",
      "Epoch 13/300\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 245514.7344 - mae: 456.7617 - val_loss: 237092.9062 - val_mae: 484.3918\n",
      "Epoch 14/300\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 245500.4219 - mae: 456.7463 - val_loss: 237082.1875 - val_mae: 484.3799\n",
      "Epoch 15/300\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 245486.0781 - mae: 456.7310 - val_loss: 237071.4062 - val_mae: 484.3679\n",
      "Epoch 16/300\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 245471.8438 - mae: 456.7158 - val_loss: 237060.5625 - val_mae: 484.3558\n",
      "Epoch 17/300\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 245457.5938 - mae: 456.7006 - val_loss: 237049.6562 - val_mae: 484.3437\n",
      "Epoch 18/300\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 245443.6094 - mae: 456.6856 - val_loss: 237038.6562 - val_mae: 484.3315\n",
      "Epoch 19/300\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 245429.8281 - mae: 456.6709 - val_loss: 237027.5625 - val_mae: 484.3192\n",
      "Epoch 20/300\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 245416.0156 - mae: 456.6563 - val_loss: 237016.4375 - val_mae: 484.3068\n",
      "Epoch 21/300\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 245402.1719 - mae: 456.6418 - val_loss: 237005.1875 - val_mae: 484.2943\n",
      "Epoch 22/300\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 245388.2344 - mae: 456.6272 - val_loss: 236993.8438 - val_mae: 484.2817\n",
      "Epoch 23/300\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 245374.2188 - mae: 456.6125 - val_loss: 236983.1250 - val_mae: 484.2698\n",
      "Epoch 24/300\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 245360.1250 - mae: 456.5976 - val_loss: 236972.8125 - val_mae: 484.2584\n",
      "Epoch 25/300\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 245345.9219 - mae: 456.5826 - val_loss: 236962.1719 - val_mae: 484.2466\n",
      "Epoch 26/300\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 245331.7188 - mae: 456.5677 - val_loss: 236951.4062 - val_mae: 484.2348\n",
      "Epoch 27/300\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 245318.1406 - mae: 456.5534 - val_loss: 236940.4688 - val_mae: 484.2228\n",
      "Epoch 28/300\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 245304.7344 - mae: 456.5391 - val_loss: 236929.2500 - val_mae: 484.2105\n",
      "Epoch 29/300\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 245291.5938 - mae: 456.5251 - val_loss: 236918.1719 - val_mae: 484.1984\n",
      "Epoch 30/300\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 245278.3594 - mae: 456.5113 - val_loss: 236907.2188 - val_mae: 484.1864\n",
      "Epoch 31/300\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 245265.0000 - mae: 456.4972 - val_loss: 236896.1250 - val_mae: 484.1743\n",
      "Epoch 32/300\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 245251.7969 - mae: 456.4832 - val_loss: 236884.8906 - val_mae: 484.1620\n",
      "Epoch 33/300\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 245238.7500 - mae: 456.4695 - val_loss: 236873.5156 - val_mae: 484.1496\n",
      "Epoch 34/300\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 245225.7031 - mae: 456.4556 - val_loss: 236861.9688 - val_mae: 484.1370\n",
      "Epoch 35/300\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 245212.5000 - mae: 456.4416 - val_loss: 236851.1250 - val_mae: 484.1252\n",
      "Epoch 36/300\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 245199.1406 - mae: 456.4275 - val_loss: 236839.9375 - val_mae: 484.1132\n",
      "Epoch 37/300\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 245185.6406 - mae: 456.4132 - val_loss: 236828.6250 - val_mae: 484.1009\n",
      "Epoch 38/300\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 245172.0000 - mae: 456.3987 - val_loss: 236817.1094 - val_mae: 484.0885\n",
      "Epoch 39/300\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 245158.3438 - mae: 456.3842 - val_loss: 236805.4375 - val_mae: 484.0759\n",
      "Epoch 40/300\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 245144.9844 - mae: 456.3698 - val_loss: 236793.5312 - val_mae: 484.0630\n",
      "Epoch 41/300\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 245131.4844 - mae: 456.3553 - val_loss: 236781.6094 - val_mae: 484.0501\n",
      "Epoch 42/300\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 245118.1562 - mae: 456.3410 - val_loss: 236769.5156 - val_mae: 484.0370\n",
      "Epoch 43/300\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 245104.8281 - mae: 456.3266 - val_loss: 236757.2500 - val_mae: 484.0237\n",
      "Epoch 44/300\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 245091.2969 - mae: 456.3120 - val_loss: 236744.8125 - val_mae: 484.0103\n",
      "Epoch 45/300\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 245077.6094 - mae: 456.2971 - val_loss: 236732.3438 - val_mae: 483.9968\n",
      "Epoch 46/300\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 245063.6719 - mae: 456.2821 - val_loss: 236719.5625 - val_mae: 483.9830\n",
      "Epoch 47/300\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 245049.6406 - mae: 456.2669 - val_loss: 236706.6094 - val_mae: 483.9691\n",
      "Epoch 48/300\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 245035.2500 - mae: 456.2515 - val_loss: 236693.3438 - val_mae: 483.9548\n",
      "Epoch 49/300\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 245020.6250 - mae: 456.2357 - val_loss: 236679.8750 - val_mae: 483.9402\n",
      "Epoch 50/300\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 245005.7812 - mae: 456.2198 - val_loss: 236666.1562 - val_mae: 483.9255\n",
      "Epoch 51/300\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 244990.7500 - mae: 456.2036 - val_loss: 236652.1562 - val_mae: 483.9104\n",
      "Epoch 52/300\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 244975.4531 - mae: 456.1872 - val_loss: 236637.5938 - val_mae: 483.8948\n",
      "Epoch 53/300\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 244960.0000 - mae: 456.1706 - val_loss: 236622.5625 - val_mae: 483.8787\n",
      "Epoch 54/300\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 244944.3906 - mae: 456.1538 - val_loss: 236607.2188 - val_mae: 483.8623\n",
      "Epoch 55/300\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 244928.6094 - mae: 456.1368 - val_loss: 236591.6250 - val_mae: 483.8457\n",
      "Epoch 56/300\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 244912.5938 - mae: 456.1196 - val_loss: 236575.8125 - val_mae: 483.8288\n",
      "Epoch 57/300\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 244896.2812 - mae: 456.1021 - val_loss: 236559.7812 - val_mae: 483.8116\n",
      "Epoch 58/300\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 244879.3906 - mae: 456.0840 - val_loss: 236543.4688 - val_mae: 483.7943\n",
      "Epoch 59/300\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 244862.2500 - mae: 456.0657 - val_loss: 236526.9062 - val_mae: 483.7766\n",
      "Epoch 60/300\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 244844.9219 - mae: 456.0471 - val_loss: 236510.2500 - val_mae: 483.7588\n",
      "Epoch 61/300\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 244827.3594 - mae: 456.0283 - val_loss: 236493.3438 - val_mae: 483.7407\n",
      "Epoch 62/300\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 244809.5469 - mae: 456.0092 - val_loss: 236476.2812 - val_mae: 483.7225\n",
      "Epoch 63/300\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 244791.4844 - mae: 455.9899 - val_loss: 236458.8906 - val_mae: 483.7039\n",
      "Epoch 64/300\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 244773.1406 - mae: 455.9703 - val_loss: 236441.2188 - val_mae: 483.6851\n",
      "Epoch 65/300\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 244754.5938 - mae: 455.9504 - val_loss: 236423.1875 - val_mae: 483.6658\n",
      "Epoch 66/300\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 244735.0938 - mae: 455.9294 - val_loss: 236404.9375 - val_mae: 483.6464\n",
      "Epoch 67/300\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 244715.0938 - mae: 455.9079 - val_loss: 236386.3750 - val_mae: 483.6266\n",
      "Epoch 68/300\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 244694.7969 - mae: 455.8860 - val_loss: 236367.5312 - val_mae: 483.6065\n",
      "Epoch 69/300\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 244674.1562 - mae: 455.8638 - val_loss: 236348.4375 - val_mae: 483.5862\n",
      "Epoch 70/300\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 244653.2188 - mae: 455.8412 - val_loss: 236329.0156 - val_mae: 483.5655\n",
      "Epoch 71/300\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 244631.9531 - mae: 455.8184 - val_loss: 236309.3125 - val_mae: 483.5445\n",
      "Epoch 72/300\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 244610.3438 - mae: 455.7951 - val_loss: 236289.2812 - val_mae: 483.5231\n",
      "Epoch 73/300\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 244588.2812 - mae: 455.7713 - val_loss: 236269.0312 - val_mae: 483.5016\n",
      "Epoch 74/300\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 244565.9062 - mae: 455.7473 - val_loss: 236248.4375 - val_mae: 483.4796\n",
      "Epoch 75/300\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 244543.0781 - mae: 455.7227 - val_loss: 236227.5312 - val_mae: 483.4574\n",
      "Epoch 76/300\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 244519.8906 - mae: 455.6977 - val_loss: 236206.3438 - val_mae: 483.4348\n",
      "Epoch 77/300\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 244496.3281 - mae: 455.6724 - val_loss: 236184.7656 - val_mae: 483.4119\n",
      "Epoch 78/300\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 244472.2969 - mae: 455.6465 - val_loss: 236162.7500 - val_mae: 483.3885\n",
      "Epoch 79/300\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 244447.8594 - mae: 455.6203 - val_loss: 236140.4375 - val_mae: 483.3647\n",
      "Epoch 80/300\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 244423.0938 - mae: 455.5935 - val_loss: 236117.8125 - val_mae: 483.3407\n",
      "Epoch 81/300\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 244397.9688 - mae: 455.5664 - val_loss: 236094.8438 - val_mae: 483.3163\n",
      "Epoch 82/300\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 244372.4219 - mae: 455.5390 - val_loss: 236071.5625 - val_mae: 483.2915\n",
      "Epoch 83/300\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 244346.5938 - mae: 455.5112 - val_loss: 236047.8750 - val_mae: 483.2664\n",
      "Epoch 84/300\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 244320.4688 - mae: 455.4829 - val_loss: 236023.5625 - val_mae: 483.2406\n",
      "Epoch 85/300\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 244293.9688 - mae: 455.4544 - val_loss: 235998.8750 - val_mae: 483.2144\n",
      "Epoch 86/300\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 244267.1094 - mae: 455.4255 - val_loss: 235973.8281 - val_mae: 483.1879\n",
      "Epoch 87/300\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 244239.8906 - mae: 455.3962 - val_loss: 235948.4375 - val_mae: 483.1609\n",
      "Epoch 88/300\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 244212.2812 - mae: 455.3664 - val_loss: 235922.6719 - val_mae: 483.1336\n",
      "Epoch 89/300\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 244184.2969 - mae: 455.3362 - val_loss: 235896.5312 - val_mae: 483.1059\n",
      "Epoch 90/300\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 244155.9844 - mae: 455.3056 - val_loss: 235870.0312 - val_mae: 483.0778\n",
      "Epoch 91/300\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 244127.2656 - mae: 455.2747 - val_loss: 235843.1406 - val_mae: 483.0493\n",
      "Epoch 92/300\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 244098.1719 - mae: 455.2432 - val_loss: 235815.8906 - val_mae: 483.0204\n",
      "Epoch 93/300\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 244068.7031 - mae: 455.2114 - val_loss: 235788.2188 - val_mae: 482.9911\n",
      "Epoch 94/300\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 244038.7500 - mae: 455.1791 - val_loss: 235760.1562 - val_mae: 482.9614\n",
      "Epoch 95/300\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 244008.3594 - mae: 455.1462 - val_loss: 235731.6562 - val_mae: 482.9312\n",
      "Epoch 96/300\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 243977.5781 - mae: 455.1129 - val_loss: 235702.7500 - val_mae: 482.9005\n",
      "Epoch 97/300\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 243946.3281 - mae: 455.0791 - val_loss: 235673.4062 - val_mae: 482.8694\n",
      "Epoch 98/300\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 243914.6562 - mae: 455.0448 - val_loss: 235643.6562 - val_mae: 482.8380\n",
      "Epoch 99/300\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 243882.5938 - mae: 455.0101 - val_loss: 235613.4688 - val_mae: 482.8060\n",
      "Epoch 100/300\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 243850.0781 - mae: 454.9748 - val_loss: 235582.8281 - val_mae: 482.7735\n",
      "Epoch 101/300\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 243817.1250 - mae: 454.9391 - val_loss: 235551.7188 - val_mae: 482.7405\n",
      "Epoch 102/300\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 243783.7500 - mae: 454.9030 - val_loss: 235520.1094 - val_mae: 482.7071\n",
      "Epoch 103/300\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 243749.9062 - mae: 454.8663 - val_loss: 235488.0156 - val_mae: 482.6731\n",
      "Epoch 104/300\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 243715.6094 - mae: 454.8291 - val_loss: 235455.4688 - val_mae: 482.6386\n",
      "Epoch 105/300\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 243680.8750 - mae: 454.7914 - val_loss: 235422.4375 - val_mae: 482.6036\n",
      "Epoch 106/300\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 243645.6406 - mae: 454.7532 - val_loss: 235388.8594 - val_mae: 482.5681\n",
      "Epoch 107/300\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 243609.9844 - mae: 454.7146 - val_loss: 235354.7188 - val_mae: 482.5319\n",
      "Epoch 108/300\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 243573.8281 - mae: 454.6753 - val_loss: 235320.1250 - val_mae: 482.4953\n",
      "Epoch 109/300\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 243537.1094 - mae: 454.6355 - val_loss: 235285.0000 - val_mae: 482.4581\n",
      "Epoch 110/300\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 243499.8906 - mae: 454.5952 - val_loss: 235249.3438 - val_mae: 482.4203\n",
      "Epoch 111/300\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 243462.1562 - mae: 454.5542 - val_loss: 235213.1875 - val_mae: 482.3821\n",
      "Epoch 112/300\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 243423.8906 - mae: 454.5127 - val_loss: 235176.4375 - val_mae: 482.3432\n",
      "Epoch 113/300\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 243385.2031 - mae: 454.4707 - val_loss: 235139.1875 - val_mae: 482.3038\n",
      "Epoch 114/300\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 243345.9531 - mae: 454.4281 - val_loss: 235101.3438 - val_mae: 482.2637\n",
      "Epoch 115/300\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 243306.2188 - mae: 454.3849 - val_loss: 235062.9844 - val_mae: 482.2231\n",
      "Epoch 116/300\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 243265.9219 - mae: 454.3412 - val_loss: 235024.0625 - val_mae: 482.1819\n",
      "Epoch 117/300\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 243225.0938 - mae: 454.2968 - val_loss: 234984.5938 - val_mae: 482.1402\n",
      "Epoch 118/300\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 243183.7188 - mae: 454.2519 - val_loss: 234944.5938 - val_mae: 482.0978\n",
      "Epoch 119/300\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 243141.6562 - mae: 454.2061 - val_loss: 234903.9062 - val_mae: 482.0548\n",
      "Epoch 120/300\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 243099.0312 - mae: 454.1599 - val_loss: 234862.5625 - val_mae: 482.0111\n",
      "Epoch 121/300\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 243055.4688 - mae: 454.1125 - val_loss: 234820.5625 - val_mae: 481.9667\n",
      "Epoch 122/300\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 243011.0312 - mae: 454.0642 - val_loss: 234777.9375 - val_mae: 481.9216\n",
      "Epoch 123/300\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 242965.9062 - mae: 454.0151 - val_loss: 234734.6250 - val_mae: 481.8759\n",
      "Epoch 124/300\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 242920.0312 - mae: 453.9651 - val_loss: 234690.6875 - val_mae: 481.8294\n",
      "Epoch 125/300\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 242873.4688 - mae: 453.9143 - val_loss: 234646.0781 - val_mae: 481.7823\n",
      "Epoch 126/300\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 242826.2188 - mae: 453.8629 - val_loss: 234600.8125 - val_mae: 481.7344\n",
      "Epoch 127/300\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 242778.2969 - mae: 453.8106 - val_loss: 234554.8438 - val_mae: 481.6859\n",
      "Epoch 128/300\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 242729.7031 - mae: 453.7576 - val_loss: 234508.1562 - val_mae: 481.6365\n",
      "Epoch 129/300\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 242680.3750 - mae: 453.7039 - val_loss: 234460.8438 - val_mae: 481.5865\n",
      "Epoch 130/300\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 242630.3906 - mae: 453.6494 - val_loss: 234412.8125 - val_mae: 481.5357\n",
      "Epoch 131/300\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 242579.6250 - mae: 453.5940 - val_loss: 234364.0625 - val_mae: 481.4843\n",
      "Epoch 132/300\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 242528.1250 - mae: 453.5378 - val_loss: 234314.6562 - val_mae: 481.4320\n",
      "Epoch 133/300\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 242475.9219 - mae: 453.4809 - val_loss: 234264.5156 - val_mae: 481.3790\n",
      "Epoch 134/300\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 242422.9531 - mae: 453.4232 - val_loss: 234212.7500 - val_mae: 481.3244\n",
      "Epoch 135/300\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 242369.2031 - mae: 453.3645 - val_loss: 234160.2188 - val_mae: 481.2690\n",
      "Epoch 136/300\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 242314.7656 - mae: 453.3052 - val_loss: 234106.9062 - val_mae: 481.2128\n",
      "Epoch 137/300\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 242259.5781 - mae: 453.2449 - val_loss: 234052.8750 - val_mae: 481.1559\n",
      "Epoch 138/300\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 242203.6406 - mae: 453.1839 - val_loss: 233998.0625 - val_mae: 481.0981\n",
      "Epoch 139/300\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 242146.9844 - mae: 453.1220 - val_loss: 233942.4375 - val_mae: 481.0394\n",
      "Epoch 140/300\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 242089.4844 - mae: 453.0588 - val_loss: 233886.0938 - val_mae: 480.9800\n",
      "Epoch 141/300\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 242031.2031 - mae: 452.9946 - val_loss: 233828.9219 - val_mae: 480.9197\n",
      "Epoch 142/300\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 241972.2031 - mae: 452.9297 - val_loss: 233770.9688 - val_mae: 480.8586\n",
      "Epoch 143/300\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 241912.3906 - mae: 452.8638 - val_loss: 233712.2500 - val_mae: 480.7968\n",
      "Epoch 144/300\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 241851.7656 - mae: 452.7970 - val_loss: 233652.7031 - val_mae: 480.7340\n",
      "Epoch 145/300\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 241790.4219 - mae: 452.7293 - val_loss: 233592.3125 - val_mae: 480.6703\n",
      "Epoch 146/300\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 241728.0469 - mae: 452.6605 - val_loss: 233531.1250 - val_mae: 480.6058\n",
      "Epoch 147/300\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 241664.8594 - mae: 452.5907 - val_loss: 233468.0781 - val_mae: 480.5393\n",
      "Epoch 148/300\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 241600.8906 - mae: 452.5200 - val_loss: 233402.6250 - val_mae: 480.4700\n",
      "Epoch 149/300\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 241536.0781 - mae: 452.4483 - val_loss: 233336.2812 - val_mae: 480.3998\n",
      "Epoch 150/300\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 241470.4062 - mae: 452.3757 - val_loss: 233269.1719 - val_mae: 480.3288\n",
      "Epoch 151/300\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 241402.7500 - mae: 452.3012 - val_loss: 233201.0625 - val_mae: 480.2567\n",
      "Epoch 152/300\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 241333.0000 - mae: 452.2246 - val_loss: 233132.0000 - val_mae: 480.1837\n",
      "Epoch 153/300\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 241262.2031 - mae: 452.1468 - val_loss: 233061.9375 - val_mae: 480.1096\n",
      "Epoch 154/300\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 241190.2969 - mae: 452.0672 - val_loss: 232990.8125 - val_mae: 480.0342\n",
      "Epoch 155/300\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 241117.3281 - mae: 451.9864 - val_loss: 232918.6406 - val_mae: 479.9578\n",
      "Epoch 156/300\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 241043.3438 - mae: 451.9045 - val_loss: 232845.4062 - val_mae: 479.8803\n",
      "Epoch 157/300\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 240968.2969 - mae: 451.8214 - val_loss: 232769.6719 - val_mae: 479.8000\n",
      "Epoch 158/300\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 240892.1719 - mae: 451.7370 - val_loss: 232691.6875 - val_mae: 479.7171\n",
      "Epoch 159/300\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 240814.7656 - mae: 451.6512 - val_loss: 232612.8750 - val_mae: 479.6334\n",
      "Epoch 160/300\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 240734.9688 - mae: 451.5632 - val_loss: 232533.0781 - val_mae: 479.5486\n",
      "Epoch 161/300\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 240653.3281 - mae: 451.4733 - val_loss: 232452.0312 - val_mae: 479.4625\n",
      "Epoch 162/300\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 240570.4219 - mae: 451.3820 - val_loss: 232369.6875 - val_mae: 479.3749\n",
      "Epoch 163/300\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 240486.2969 - mae: 451.2893 - val_loss: 232286.1250 - val_mae: 479.2860\n",
      "Epoch 164/300\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 240400.9688 - mae: 451.1953 - val_loss: 232201.3281 - val_mae: 479.1959\n",
      "Epoch 165/300\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 240314.4688 - mae: 451.0999 - val_loss: 232115.3125 - val_mae: 479.1044\n",
      "Epoch 166/300\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 240226.8438 - mae: 451.0031 - val_loss: 232028.0938 - val_mae: 479.0116\n",
      "Epoch 167/300\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 240137.9688 - mae: 450.9051 - val_loss: 231939.7031 - val_mae: 478.9175\n",
      "Epoch 168/300\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 240047.9844 - mae: 450.8058 - val_loss: 231850.1094 - val_mae: 478.8221\n",
      "Epoch 169/300\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 239956.8281 - mae: 450.7051 - val_loss: 231759.3438 - val_mae: 478.7255\n",
      "Epoch 170/300\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 239864.5312 - mae: 450.6032 - val_loss: 231667.2812 - val_mae: 478.6274\n",
      "Epoch 171/300\n",
      "1/1 [==============================] - 0s 127ms/step - loss: 239771.0781 - mae: 450.4999 - val_loss: 231573.9844 - val_mae: 478.5281\n",
      "Epoch 172/300\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 239676.4688 - mae: 450.3953 - val_loss: 231479.4688 - val_mae: 478.4274\n",
      "Epoch 173/300\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 239580.6562 - mae: 450.2894 - val_loss: 231383.8125 - val_mae: 478.3254\n",
      "Epoch 174/300\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 239483.7656 - mae: 450.1822 - val_loss: 231286.9062 - val_mae: 478.2221\n",
      "Epoch 175/300\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 239385.6719 - mae: 450.0737 - val_loss: 231188.8438 - val_mae: 478.1176\n",
      "Epoch 176/300\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 239286.4531 - mae: 449.9638 - val_loss: 231089.3906 - val_mae: 478.0115\n",
      "Epoch 177/300\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 239186.0000 - mae: 449.8527 - val_loss: 230988.4375 - val_mae: 477.9039\n",
      "Epoch 178/300\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 239084.4219 - mae: 449.7401 - val_loss: 230886.2656 - val_mae: 477.7950\n",
      "Epoch 179/300\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 238981.6562 - mae: 449.6262 - val_loss: 230782.8750 - val_mae: 477.6847\n",
      "Epoch 180/300\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 238877.7188 - mae: 449.5110 - val_loss: 230678.2031 - val_mae: 477.5730\n",
      "Epoch 181/300\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 238772.5938 - mae: 449.3944 - val_loss: 230572.2812 - val_mae: 477.4600\n",
      "Epoch 182/300\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 238666.2656 - mae: 449.2765 - val_loss: 230464.8750 - val_mae: 477.3453\n",
      "Epoch 183/300\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 238558.6094 - mae: 449.1570 - val_loss: 230356.1406 - val_mae: 477.2292\n",
      "Epoch 184/300\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 238449.6406 - mae: 449.0361 - val_loss: 230246.1094 - val_mae: 477.1117\n",
      "Epoch 185/300\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 238339.2656 - mae: 448.9137 - val_loss: 230134.7031 - val_mae: 476.9927\n",
      "Epoch 186/300\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 238227.6250 - mae: 448.7898 - val_loss: 230021.9688 - val_mae: 476.8723\n",
      "Epoch 187/300\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 238114.6719 - mae: 448.6644 - val_loss: 229907.8750 - val_mae: 476.7503\n",
      "Epoch 188/300\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 238000.4688 - mae: 448.5375 - val_loss: 229792.3750 - val_mae: 476.6268\n",
      "Epoch 189/300\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 237884.7656 - mae: 448.4090 - val_loss: 229675.3438 - val_mae: 476.5017\n",
      "Epoch 190/300\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 237767.7344 - mae: 448.2790 - val_loss: 229556.6875 - val_mae: 476.3749\n",
      "Epoch 191/300\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 237649.3750 - mae: 448.1476 - val_loss: 229436.6094 - val_mae: 476.2465\n",
      "Epoch 192/300\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 237529.7500 - mae: 448.0146 - val_loss: 229315.1250 - val_mae: 476.1165\n",
      "Epoch 193/300\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 237408.7812 - mae: 447.8800 - val_loss: 229192.2500 - val_mae: 475.9850\n",
      "Epoch 194/300\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 237286.5312 - mae: 447.7440 - val_loss: 229067.9375 - val_mae: 475.8521\n",
      "Epoch 195/300\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 237162.9219 - mae: 447.6064 - val_loss: 228942.2500 - val_mae: 475.7175\n",
      "Epoch 196/300\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 237038.0469 - mae: 447.4673 - val_loss: 228815.1250 - val_mae: 475.5814\n",
      "Epoch 197/300\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 236911.7969 - mae: 447.3266 - val_loss: 228686.5938 - val_mae: 475.4437\n",
      "Epoch 198/300\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 236784.2344 - mae: 447.1844 - val_loss: 228556.6562 - val_mae: 475.3045\n",
      "Epoch 199/300\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 236655.3594 - mae: 447.0408 - val_loss: 228425.2812 - val_mae: 475.1637\n",
      "Epoch 200/300\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 236525.0000 - mae: 446.8954 - val_loss: 228292.4531 - val_mae: 475.0213\n",
      "Epoch 201/300\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 236393.2188 - mae: 446.7484 - val_loss: 228158.1250 - val_mae: 474.8773\n",
      "Epoch 202/300\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 236259.9531 - mae: 446.5997 - val_loss: 228022.2812 - val_mae: 474.7316\n",
      "Epoch 203/300\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 236125.3594 - mae: 446.4495 - val_loss: 227885.0000 - val_mae: 474.5843\n",
      "Epoch 204/300\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 235989.3438 - mae: 446.2976 - val_loss: 227746.1719 - val_mae: 474.4353\n",
      "Epoch 205/300\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 235851.7031 - mae: 446.1439 - val_loss: 227605.7812 - val_mae: 474.2846\n",
      "Epoch 206/300\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 235712.6406 - mae: 445.9886 - val_loss: 227463.8750 - val_mae: 474.1322\n",
      "Epoch 207/300\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 235572.1094 - mae: 445.8315 - val_loss: 227320.4688 - val_mae: 473.9782\n",
      "Epoch 208/300\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 235430.2031 - mae: 445.6729 - val_loss: 227175.4844 - val_mae: 473.8224\n",
      "Epoch 209/300\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 235286.8594 - mae: 445.5125 - val_loss: 227029.0312 - val_mae: 473.6650\n",
      "Epoch 210/300\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 235142.1250 - mae: 445.3505 - val_loss: 226881.0312 - val_mae: 473.5059\n",
      "Epoch 211/300\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 234995.9219 - mae: 445.1868 - val_loss: 226731.5000 - val_mae: 473.3450\n",
      "Epoch 212/300\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 234848.2656 - mae: 445.0214 - val_loss: 226580.4688 - val_mae: 473.1825\n",
      "Epoch 213/300\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 234699.2500 - mae: 444.8544 - val_loss: 226427.9062 - val_mae: 473.0183\n",
      "Epoch 214/300\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 234548.7344 - mae: 444.6857 - val_loss: 226273.8438 - val_mae: 472.8524\n",
      "Epoch 215/300\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 234396.8281 - mae: 444.5153 - val_loss: 226118.2188 - val_mae: 472.6848\n",
      "Epoch 216/300\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 234243.2969 - mae: 444.3430 - val_loss: 225961.0625 - val_mae: 472.5154\n",
      "Epoch 217/300\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 234088.2969 - mae: 444.1691 - val_loss: 225802.3125 - val_mae: 472.3444\n",
      "Epoch 218/300\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 233931.8594 - mae: 443.9932 - val_loss: 225642.0156 - val_mae: 472.1715\n",
      "Epoch 219/300\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 233773.8906 - mae: 443.8156 - val_loss: 225480.1562 - val_mae: 471.9969\n",
      "Epoch 220/300\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 233614.4531 - mae: 443.6361 - val_loss: 225316.7031 - val_mae: 471.8206\n",
      "Epoch 221/300\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 233453.5000 - mae: 443.4548 - val_loss: 225151.6875 - val_mae: 471.6424\n",
      "Epoch 222/300\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 233291.0469 - mae: 443.2717 - val_loss: 224984.9375 - val_mae: 471.4623\n",
      "Epoch 223/300\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 233127.1719 - mae: 443.0868 - val_loss: 224816.3438 - val_mae: 471.2803\n",
      "Epoch 224/300\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 232961.7656 - mae: 442.9001 - val_loss: 224646.1719 - val_mae: 471.0964\n",
      "Epoch 225/300\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 232794.8594 - mae: 442.7117 - val_loss: 224474.3750 - val_mae: 470.9108\n",
      "Epoch 226/300\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 232626.4219 - mae: 442.5215 - val_loss: 224301.0000 - val_mae: 470.7233\n",
      "Epoch 227/300\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 232456.5469 - mae: 442.3294 - val_loss: 224126.0000 - val_mae: 470.5340\n",
      "Epoch 228/300\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 232285.1406 - mae: 442.1356 - val_loss: 223949.4062 - val_mae: 470.3429\n",
      "Epoch 229/300\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 232112.2500 - mae: 441.9400 - val_loss: 223771.2031 - val_mae: 470.1500\n",
      "Epoch 230/300\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 231937.8594 - mae: 441.7426 - val_loss: 223591.3750 - val_mae: 469.9552\n",
      "Epoch 231/300\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 231761.9062 - mae: 441.5433 - val_loss: 223409.9375 - val_mae: 469.7586\n",
      "Epoch 232/300\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 231584.4688 - mae: 441.3422 - val_loss: 223226.8125 - val_mae: 469.5602\n",
      "Epoch 233/300\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 231405.5156 - mae: 441.1393 - val_loss: 223042.1094 - val_mae: 469.3599\n",
      "Epoch 234/300\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 231224.9844 - mae: 440.9345 - val_loss: 222855.7656 - val_mae: 469.1577\n",
      "Epoch 235/300\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 231043.0000 - mae: 440.7280 - val_loss: 222667.7812 - val_mae: 468.9537\n",
      "Epoch 236/300\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 230859.4219 - mae: 440.5195 - val_loss: 222478.1875 - val_mae: 468.7478\n",
      "Epoch 237/300\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 230674.3438 - mae: 440.3091 - val_loss: 222286.9062 - val_mae: 468.5400\n",
      "Epoch 238/300\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 230487.7344 - mae: 440.0969 - val_loss: 222094.0000 - val_mae: 468.3303\n",
      "Epoch 239/300\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 230299.4531 - mae: 439.8828 - val_loss: 221899.3750 - val_mae: 468.1187\n",
      "Epoch 240/300\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 230109.5312 - mae: 439.6666 - val_loss: 221703.0469 - val_mae: 467.9051\n",
      "Epoch 241/300\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 229918.0938 - mae: 439.4485 - val_loss: 221505.0625 - val_mae: 467.6896\n",
      "Epoch 242/300\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 229725.0312 - mae: 439.2286 - val_loss: 221305.3438 - val_mae: 467.4722\n",
      "Epoch 243/300\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 229530.3438 - mae: 439.0066 - val_loss: 221103.9375 - val_mae: 467.2528\n",
      "Epoch 244/300\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 229333.9688 - mae: 438.7827 - val_loss: 220900.7656 - val_mae: 467.0314\n",
      "Epoch 245/300\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 229136.0312 - mae: 438.5567 - val_loss: 220695.8906 - val_mae: 466.8080\n",
      "Epoch 246/300\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 228936.4688 - mae: 438.3288 - val_loss: 220489.3125 - val_mae: 466.5826\n",
      "Epoch 247/300\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 228735.2656 - mae: 438.0989 - val_loss: 220280.9531 - val_mae: 466.3552\n",
      "Epoch 248/300\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 228532.5000 - mae: 437.8671 - val_loss: 220070.9062 - val_mae: 466.1259\n",
      "Epoch 249/300\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 228328.0312 - mae: 437.6332 - val_loss: 219859.0938 - val_mae: 465.8945\n",
      "Epoch 250/300\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 228121.9062 - mae: 437.3972 - val_loss: 219645.5625 - val_mae: 465.6611\n",
      "Epoch 251/300\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 227914.1250 - mae: 437.1593 - val_loss: 219430.2188 - val_mae: 465.4256\n",
      "Epoch 252/300\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 227704.7344 - mae: 436.9193 - val_loss: 219213.1875 - val_mae: 465.1882\n",
      "Epoch 253/300\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 227493.7344 - mae: 436.6774 - val_loss: 218994.3750 - val_mae: 464.9487\n",
      "Epoch 254/300\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 227281.0469 - mae: 436.4334 - val_loss: 218773.8281 - val_mae: 464.7072\n",
      "Epoch 255/300\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 227066.7969 - mae: 436.1874 - val_loss: 218551.2500 - val_mae: 464.4632\n",
      "Epoch 256/300\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 226850.9219 - mae: 435.9394 - val_loss: 218326.9062 - val_mae: 464.2172\n",
      "Epoch 257/300\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 226633.4219 - mae: 435.6894 - val_loss: 218100.7656 - val_mae: 463.9691\n",
      "Epoch 258/300\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 226414.2656 - mae: 435.4372 - val_loss: 217872.8750 - val_mae: 463.7190\n",
      "Epoch 259/300\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 226193.5156 - mae: 435.1831 - val_loss: 217643.2500 - val_mae: 463.4667\n",
      "Epoch 260/300\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 225971.1406 - mae: 434.9268 - val_loss: 217411.8438 - val_mae: 463.2125\n",
      "Epoch 261/300\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 225747.1094 - mae: 434.6686 - val_loss: 217178.6875 - val_mae: 462.9561\n",
      "Epoch 262/300\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 225521.4844 - mae: 434.4083 - val_loss: 216943.7812 - val_mae: 462.6977\n",
      "Epoch 263/300\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 225294.1719 - mae: 434.1460 - val_loss: 216707.0938 - val_mae: 462.4371\n",
      "Epoch 264/300\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 225065.2812 - mae: 433.8815 - val_loss: 216468.6875 - val_mae: 462.1745\n",
      "Epoch 265/300\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 224834.7656 - mae: 433.6150 - val_loss: 216228.4844 - val_mae: 461.9098\n",
      "Epoch 266/300\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 224602.6094 - mae: 433.3464 - val_loss: 215986.5312 - val_mae: 461.6429\n",
      "Epoch 267/300\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 224368.7969 - mae: 433.0758 - val_loss: 215742.7812 - val_mae: 461.3740\n",
      "Epoch 268/300\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 224133.3750 - mae: 432.8030 - val_loss: 215497.2969 - val_mae: 461.1030\n",
      "Epoch 269/300\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 223896.3281 - mae: 432.5281 - val_loss: 215250.0312 - val_mae: 460.8298\n",
      "Epoch 270/300\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 223657.6250 - mae: 432.2512 - val_loss: 215000.9688 - val_mae: 460.5544\n",
      "Epoch 271/300\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 223417.2812 - mae: 431.9721 - val_loss: 214750.1406 - val_mae: 460.2770\n",
      "Epoch 272/300\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 223175.2812 - mae: 431.6909 - val_loss: 214497.5312 - val_mae: 459.9974\n",
      "Epoch 273/300\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 222931.6719 - mae: 431.4077 - val_loss: 214243.1875 - val_mae: 459.7157\n",
      "Epoch 274/300\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 222686.4062 - mae: 431.1223 - val_loss: 213987.0000 - val_mae: 459.4318\n",
      "Epoch 275/300\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 222439.4844 - mae: 430.8347 - val_loss: 213729.0469 - val_mae: 459.1457\n",
      "Epoch 276/300\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 222190.9219 - mae: 430.5450 - val_loss: 213469.3125 - val_mae: 458.8575\n",
      "Epoch 277/300\n",
      "1/1 [==============================] - 0s 120ms/step - loss: 221940.7188 - mae: 430.2531 - val_loss: 213207.7812 - val_mae: 458.5670\n",
      "Epoch 278/300\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 221688.8750 - mae: 429.9592 - val_loss: 212944.5000 - val_mae: 458.2744\n",
      "Epoch 279/300\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 221435.3594 - mae: 429.6630 - val_loss: 212679.3750 - val_mae: 457.9796\n",
      "Epoch 280/300\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 221180.2188 - mae: 429.3647 - val_loss: 212412.4844 - val_mae: 457.6827\n",
      "Epoch 281/300\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 220923.4219 - mae: 429.0643 - val_loss: 212143.8125 - val_mae: 457.3835\n",
      "Epoch 282/300\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 220664.9688 - mae: 428.7616 - val_loss: 211873.3281 - val_mae: 457.0822\n",
      "Epoch 283/300\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 220404.8281 - mae: 428.4568 - val_loss: 211600.9375 - val_mae: 456.7784\n",
      "Epoch 284/300\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 220142.6406 - mae: 428.1494 - val_loss: 211326.5781 - val_mae: 456.4723\n",
      "Epoch 285/300\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 219878.7500 - mae: 427.8398 - val_loss: 211049.9062 - val_mae: 456.1633\n",
      "Epoch 286/300\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 219613.1562 - mae: 427.5279 - val_loss: 210771.3125 - val_mae: 455.8520\n",
      "Epoch 287/300\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 219345.8594 - mae: 427.2137 - val_loss: 210490.8438 - val_mae: 455.5383\n",
      "Epoch 288/300\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 219076.6250 - mae: 426.8971 - val_loss: 210208.4688 - val_mae: 455.2223\n",
      "Epoch 289/300\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 218805.7188 - mae: 426.5782 - val_loss: 209924.1719 - val_mae: 454.9039\n",
      "Epoch 290/300\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 218533.0469 - mae: 426.2570 - val_loss: 209638.0156 - val_mae: 454.5832\n",
      "Epoch 291/300\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 218258.6562 - mae: 425.9336 - val_loss: 209349.9688 - val_mae: 454.2602\n",
      "Epoch 292/300\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 217982.6406 - mae: 425.6078 - val_loss: 209060.0938 - val_mae: 453.9348\n",
      "Epoch 293/300\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 217704.8594 - mae: 425.2798 - val_loss: 208768.3125 - val_mae: 453.6071\n",
      "Epoch 294/300\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 217425.3750 - mae: 424.9494 - val_loss: 208474.7031 - val_mae: 453.2771\n",
      "Epoch 295/300\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 217144.2500 - mae: 424.6169 - val_loss: 208179.2500 - val_mae: 452.9448\n",
      "Epoch 296/300\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 216861.3594 - mae: 424.2819 - val_loss: 207881.9375 - val_mae: 452.6101\n",
      "Epoch 297/300\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 216576.8594 - mae: 423.9447 - val_loss: 207582.7812 - val_mae: 452.2731\n",
      "Epoch 298/300\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 216290.6562 - mae: 423.6052 - val_loss: 207281.8125 - val_mae: 451.9337\n",
      "Epoch 299/300\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 216002.7656 - mae: 423.2634 - val_loss: 206979.0000 - val_mae: 451.5920\n",
      "Epoch 300/300\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 215713.1094 - mae: 422.9191 - val_loss: 206674.2812 - val_mae: 451.2480\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\NXTWAVE\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 131ms/step\n",
      "MAE  : 345.2377738952637\n",
      "RMSE : 352.0726103329634\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "ALL FILES GENERATED SUCCESSFULLY \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
    "import json\n",
    "import yaml\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# ============================\n",
    "# PATH CONFIG\n",
    "# ============================\n",
    "\n",
    "BASE_PATH = r\"C:\\Users\\NXTWAVE\\Downloads\\Justice Delay Analytics for Senior Citizen Cases (Karnataka HC)\"\n",
    "CSV_PATH = os.path.join(\n",
    "    BASE_PATH,\n",
    "    \"casesdisposedofseniorcitizenHCKBengaluru_3.csv\"\n",
    ")\n",
    "\n",
    "MODEL_PATH = os.path.join(BASE_PATH, \"justice_delay_model.h5\")\n",
    "SCALER_PATH = os.path.join(BASE_PATH, \"justice_delay_scaler.pkl\")\n",
    "CONFIG_PATH = os.path.join(BASE_PATH, \"justice_delay_config.yaml\")\n",
    "PREDICTION_PATH = os.path.join(BASE_PATH, \"justice_delay_predictions.json\")\n",
    "\n",
    "# ============================\n",
    "# LOAD DATA\n",
    "# ============================\n",
    "\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "print(\"Dataset Loaded Successfully\")\n",
    "print(df.head())\n",
    "\n",
    "# ============================\n",
    "# CLEAN COLUMN NAMES\n",
    "# ============================\n",
    "\n",
    "df.columns = [c.strip().lower().replace(\".\", \"\").replace(\" \", \"_\") for c in df.columns]\n",
    "\n",
    "# Keep only required columns\n",
    "df = df[[\"month\", \"year\", \"count\"]].dropna()\n",
    "\n",
    "df[\"year\"] = df[\"year\"].astype(int)\n",
    "df[\"count\"] = df[\"count\"].astype(float)\n",
    "\n",
    "# ============================\n",
    "# FIX MONTH STRINGS (IMPORTANT)\n",
    "# ============================\n",
    "\n",
    "df[\"month\"] = df[\"month\"].astype(str).str.strip().str.title()\n",
    "\n",
    "month_map = {\n",
    "    \"January\": 1, \"February\": 2, \"March\": 3,\n",
    "    \"April\": 4, \"May\": 5, \"June\": 6,\n",
    "    \"July\": 7, \"August\": 8, \"September\": 9,\n",
    "    \"October\": 10, \"November\": 11, \"December\": 12\n",
    "}\n",
    "\n",
    "df[\"month_num\"] = df[\"month\"].map(month_map)\n",
    "\n",
    "# Drop rows with invalid months (safety)\n",
    "df = df.dropna(subset=[\"month_num\"])\n",
    "df[\"month_num\"] = df[\"month_num\"].astype(int)\n",
    "\n",
    "# ============================\n",
    "# FEATURES & TARGET\n",
    "# ============================\n",
    "\n",
    "X = df[[\"year\", \"month_num\"]].values\n",
    "y = df[\"count\"].values\n",
    "\n",
    "# ============================\n",
    "# SCALING\n",
    "# ============================\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "with open(SCALER_PATH, \"wb\") as f:\n",
    "    pickle.dump(scaler, f)\n",
    "\n",
    "# ============================\n",
    "# TRAIN TEST SPLIT\n",
    "# ============================\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# ============================\n",
    "# MODEL\n",
    "# ============================\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(32, activation=\"relu\", input_shape=(X_train.shape[1],)),\n",
    "    Dense(16, activation=\"relu\"),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"mse\",\n",
    "    metrics=[\"mae\"]\n",
    ")\n",
    "\n",
    "early_stop = EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    patience=20,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "# ============================\n",
    "# TRAINING\n",
    "# ============================\n",
    "\n",
    "model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    validation_split=0.2,\n",
    "    epochs=300,\n",
    "    batch_size=8,\n",
    "    callbacks=[early_stop],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# ============================\n",
    "# SAVE MODEL\n",
    "# ============================\n",
    "\n",
    "model.save(MODEL_PATH)\n",
    "\n",
    "# ============================\n",
    "# EVALUATION\n",
    "# ============================\n",
    "\n",
    "y_pred = model.predict(X_test).flatten()\n",
    "\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "print(f\"MAE  : {mae}\")\n",
    "print(f\"RMSE : {rmse}\")\n",
    "\n",
    "# ============================\n",
    "# SAVE YAML CONFIG\n",
    "# ============================\n",
    "\n",
    "config = {\n",
    "    \"project\": \"Justice Delay Analytics for Senior Citizen Cases\",\n",
    "    \"court\": \"Karnataka High Court  Bengaluru\",\n",
    "    \"model\": \"Neural Network (MLP)\",\n",
    "    \"features\": [\"year\", \"month_num\"],\n",
    "    \"target\": \"count\",\n",
    "    \"metrics\": {\n",
    "        \"MAE\": float(mae),\n",
    "        \"RMSE\": float(rmse)\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(CONFIG_PATH, \"w\") as f:\n",
    "    yaml.dump(config, f)\n",
    "\n",
    "# ============================\n",
    "# FUTURE PREDICTIONS (NEXT 12 MONTHS)\n",
    "# ============================\n",
    "\n",
    "last_year = df[\"year\"].max()\n",
    "\n",
    "future_data = []\n",
    "for m in range(1, 13):\n",
    "    future_data.append([last_year + 1, m])\n",
    "\n",
    "future_df = pd.DataFrame(future_data, columns=[\"year\", \"month_num\"])\n",
    "future_scaled = scaler.transform(future_df.values)\n",
    "future_preds = model.predict(future_scaled).flatten()\n",
    "\n",
    "prediction_output = {\n",
    "    \"future_predictions\": [\n",
    "        {\n",
    "            \"year\": int(y),\n",
    "            \"month\": int(m),\n",
    "            \"predicted_cases_disposed\": float(p)\n",
    "        }\n",
    "        for (y, m), p in zip(future_data, future_preds)\n",
    "    ]\n",
    "}\n",
    "\n",
    "with open(PREDICTION_PATH, \"w\") as f:\n",
    "    json.dump(prediction_output, f, indent=4)\n",
    "\n",
    "print(\"ALL FILES GENERATED SUCCESSFULLY \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd06309-6518-4eb6-9fde-79d753335493",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
